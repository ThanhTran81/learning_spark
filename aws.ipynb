{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_key='AKIAJARJYVIZOYG2THBA'\n",
    "secret='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'\n",
    "bucket_name='thanh-learn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# make sure pyspark tells workers to use python2 \n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret\n",
    "SUBMIT_ARGS = \"--jars file:/C:/MyProgz/hadoop-2.7.3/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar,\" \\\n",
    "              \"file:/C:/MyProgz/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-aws-2.7.3.jar  pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x0000000004697630>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "sparkConf = SparkConf().setAppName(\"learn\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=sparkConf)\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.functions import lit\n",
    "import boto\n",
    "from urlparse import urlsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dispatching_base_number: string (nullable = true)\n |-- date: string (nullable = true)\n |-- active_vehicles: integer (nullable = true)\n |-- trips: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "file = 'file:///Users/Tran/git/_learning/learning_spark/data/uber.csv'\n",
    "file_df = sqlContext.read.load(file, format='com.databricks.spark.csv', header='true', inferSchema='true')\n",
    "file_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the content of uber_file\n"
     ]
    }
   ],
   "source": [
    "from boto.s3.key import Key\n",
    "con_s3 = boto.connect_s3(access_key,secret)\n",
    "bucket = con_s3.get_bucket(bucket_name=bucket_name)\n",
    "k = Key(bucket)\n",
    "k.key = 'uber_file'\n",
    "k.set_contents_from_string('this is the content of uber_file')\n",
    "print (k.get_contents_as_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Key: thanh-learn,uber_test_file>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.key = 'uber_test_file'\n",
    "# create object from file on s3\n",
    "k.set_contents_from_filename('C:\\Users\\Tran\\git\\_learning\\learning_spark\\data\\uber.csv')\n",
    "# copy object to local file\n",
    "k.get_contents_to_filename('C:\\Users\\Tran\\git\\_learning\\learning_spark\\data\\uber_test.csv')\n",
    "k.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket: <Bucket: aws-logs-625183434717-us-east-1>\nbucket: <Bucket: aws-logs-625183434717-us-west-2>\nbucket: <Bucket: elasticbeanstalk-us-east-1-625183434717>\nbucket: <Bucket: knowie>\nbucket: <Bucket: lyfeline-emr>\nbucket: <Bucket: lyfeline-media>\nbucket: <Bucket: lyfeline-media-development>\nbucket: <Bucket: lyfeline-media-production>\nbucket: <Bucket: lyfeline-media-staging>\nbucket: <Bucket: lyfeline-media-test>\nbucket: <Bucket: milestone-media>\nbucket: <Bucket: milestone-media-development>\nbucket: <Bucket: milestone-media-production>\nbucket: <Bucket: milestone-media-staging>\nbucket: <Bucket: milestone-media-test>\nbucket: <Bucket: milestone-user-event>\nbucket: <Bucket: milestone-website-media>\nbucket: <Bucket: milestones-trail>\nbucket: <Bucket: thanh-learn>\nkey in bucket: <Key: thanh-learn,uber.csv>\nkey in bucket: <Key: thanh-learn,uber_file>\n"
     ]
    }
   ],
   "source": [
    "buckets = con_s3.get_all_buckets()\n",
    "for b in buckets:\n",
    "    print ('bucket: %s' % b)\n",
    "for i in bucket.list():\n",
    "    print('key in bucket: %s' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret)\n",
    "filename_csv = 'uber.csv'\n",
    "s3_file_csv_uri = \"s3a://{}/{}\".format(bucket_name, filename)\n",
    "filename_txt = 'uber_file'\n",
    "s3_file_txt_uri = \"s3a://{}/{}\".format(bucket_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read as text [u'dispatching_base_number,date,active_vehicles,trips', u'B02512,1/1/2015,190,1132', u'B02765,1/1/2015,225,1765']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dispatching_base_number: string (nullable = true)\n |-- date: string (nullable = true)\n |-- active_vehicles: integer (nullable = true)\n |-- trips: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "s3_file_txt = sc.textFile(s3_file_txt_uri)\n",
    "print 'File read as text, first 3 lines: {}'.format(s3_file_txt.take(3))\n",
    "s3_file_df = sqlContext.read.load(s3_file_csv_uri, format='com.databricks.spark.csv', header='true', inferSchema='true')\n",
    "s3_file_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}